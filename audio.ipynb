{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0 Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math, random\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder\n",
    "\n",
    "from vscode_audio import Audio\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "from torch.nn import init\n",
    "import torchaudio\n",
    "from torchaudio import transforms\n",
    "\n",
    "plt.rcParams['figure.facecolor'] = 'white'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = pd.read_csv(\"data/train_post_competition.csv\")\n",
    "data_test = pd.read_csv(\"data/test_post_competition.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 9473 entries, 0 to 9472\n",
      "Data columns (total 5 columns):\n",
      " #   Column             Non-Null Count  Dtype \n",
      "---  ------             --------------  ----- \n",
      " 0   fname              9473 non-null   object\n",
      " 1   label              9473 non-null   object\n",
      " 2   manually_verified  9473 non-null   int64 \n",
      " 3   freesound_id       9473 non-null   int64 \n",
      " 4   license            9473 non-null   object\n",
      "dtypes: int64(2), object(3)\n",
      "memory usage: 370.2+ KB\n"
     ]
    }
   ],
   "source": [
    "data_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fname</th>\n",
       "      <th>label</th>\n",
       "      <th>manually_verified</th>\n",
       "      <th>freesound_id</th>\n",
       "      <th>license</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00044347.wav</td>\n",
       "      <td>Hi-hat</td>\n",
       "      <td>0</td>\n",
       "      <td>28739</td>\n",
       "      <td>Attribution</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>001ca53d.wav</td>\n",
       "      <td>Saxophone</td>\n",
       "      <td>1</td>\n",
       "      <td>358827</td>\n",
       "      <td>Attribution</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>002d256b.wav</td>\n",
       "      <td>Trumpet</td>\n",
       "      <td>0</td>\n",
       "      <td>10897</td>\n",
       "      <td>Creative Commons 0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0033e230.wav</td>\n",
       "      <td>Glockenspiel</td>\n",
       "      <td>1</td>\n",
       "      <td>325017</td>\n",
       "      <td>Attribution</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00353774.wav</td>\n",
       "      <td>Cello</td>\n",
       "      <td>1</td>\n",
       "      <td>195688</td>\n",
       "      <td>Attribution</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          fname         label  manually_verified  freesound_id  \\\n",
       "0  00044347.wav        Hi-hat                  0         28739   \n",
       "1  001ca53d.wav     Saxophone                  1        358827   \n",
       "2  002d256b.wav       Trumpet                  0         10897   \n",
       "3  0033e230.wav  Glockenspiel                  1        325017   \n",
       "4  00353774.wav         Cello                  1        195688   \n",
       "\n",
       "              license  \n",
       "0         Attribution  \n",
       "1         Attribution  \n",
       "2  Creative Commons 0  \n",
       "3         Attribution  \n",
       "4         Attribution  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 9400 entries, 0 to 9399\n",
      "Data columns (total 5 columns):\n",
      " #   Column        Non-Null Count  Dtype \n",
      "---  ------        --------------  ----- \n",
      " 0   fname         9400 non-null   object\n",
      " 1   label         9400 non-null   object\n",
      " 2   usage         9400 non-null   object\n",
      " 3   freesound_id  9400 non-null   int64 \n",
      " 4   license       9400 non-null   object\n",
      "dtypes: int64(1), object(4)\n",
      "memory usage: 367.3+ KB\n"
     ]
    }
   ],
   "source": [
    "data_test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fname</th>\n",
       "      <th>label</th>\n",
       "      <th>usage</th>\n",
       "      <th>freesound_id</th>\n",
       "      <th>license</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00063640.wav</td>\n",
       "      <td>None</td>\n",
       "      <td>Ignored</td>\n",
       "      <td>88926</td>\n",
       "      <td>Attribution</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0013a1db.wav</td>\n",
       "      <td>None</td>\n",
       "      <td>Ignored</td>\n",
       "      <td>373335</td>\n",
       "      <td>Creative Commons 0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>002bb878.wav</td>\n",
       "      <td>None</td>\n",
       "      <td>Ignored</td>\n",
       "      <td>189611</td>\n",
       "      <td>Attribution</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>002d392d.wav</td>\n",
       "      <td>None</td>\n",
       "      <td>Ignored</td>\n",
       "      <td>35939</td>\n",
       "      <td>Attribution</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00326aa9.wav</td>\n",
       "      <td>Oboe</td>\n",
       "      <td>Private</td>\n",
       "      <td>355125</td>\n",
       "      <td>Attribution</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          fname label    usage  freesound_id             license\n",
       "0  00063640.wav  None  Ignored         88926         Attribution\n",
       "1  0013a1db.wav  None  Ignored        373335  Creative Commons 0\n",
       "2  002bb878.wav  None  Ignored        189611         Attribution\n",
       "3  002d392d.wav  None  Ignored         35939         Attribution\n",
       "4  00326aa9.wav  Oboe  Private        355125         Attribution"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train.label.unique().size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_test.label.unique().size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train.drop(columns=[\"manually_verified\", \"freesound_id\", \"license\"], inplace=True)\n",
    "data_test.drop(columns=[\"usage\", \"freesound_id\", \"license\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 9473 entries, 0 to 9472\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   fname   9473 non-null   object\n",
      " 1   label   9473 non-null   object\n",
      "dtypes: object(2)\n",
      "memory usage: 148.1+ KB\n"
     ]
    }
   ],
   "source": [
    "data_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 9400 entries, 0 to 9399\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   fname   9400 non-null   object\n",
      " 1   label   9400 non-null   object\n",
      "dtypes: object(2)\n",
      "memory usage: 147.0+ KB\n"
     ]
    }
   ],
   "source": [
    "data_test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enc = OrdinalEncoder(dtype=np.int64, handle_unknown=\"use_encoded_value\", unknown_value=np.nan)\n",
    "enc = OrdinalEncoder(dtype=np.int64)\n",
    "enc.fit(data_test[[\"label\"]])\n",
    "data_train[\"label_enc\"] = enc.transform(data_train[[\"label\"]])\n",
    "data_test[\"label_enc\"] = enc.transform(data_test[[\"label\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LABELS_COUNT = data_test[\"label_enc\"].unique().size\n",
    "LABELS_COUNT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioUtils():\n",
    "    @staticmethod\n",
    "    def open(audio_file):\n",
    "        sig, sr = torchaudio.load(audio_file)\n",
    "        return (sig, sr)\n",
    "\n",
    "    @staticmethod\n",
    "    def rechannel(audio, new_channel):\n",
    "        sig, sr = audio\n",
    "        if sig.shape[0] == new_channel:\n",
    "            return audio\n",
    "        if new_channel == 1:\n",
    "            resig = sig[:1, :]\n",
    "        else:\n",
    "            resig = torch.cat([sig, sig])\n",
    "        return resig, sr\n",
    "\n",
    "    @staticmethod\n",
    "    def resample(audio, newsr):\n",
    "        sig, sr = audio\n",
    "        if sr == newsr:\n",
    "            return audio\n",
    "        num_channels = sig.shape[0]\n",
    "        resig = torchaudio.transforms.Resample(sr, newsr)(sig[:1, :])\n",
    "        if num_channels > 1:\n",
    "            retwo = torchaudio.transforms.Resample(sr, newsr)(sig[1:, :])\n",
    "            resig = torch.cat([resig, retwo])\n",
    "        return resig, newsr\n",
    "\n",
    "    @staticmethod\n",
    "    def pad_trunc(audio, max_ms):\n",
    "        sig, sr = audio\n",
    "        num_rows, sig_len = sig.shape\n",
    "        max_len = sr//1000 * max_ms\n",
    "        if sig_len > max_len:\n",
    "            sig = sig[:, :max_len]\n",
    "        elif sig_len < max_len:\n",
    "            pad_begin_len = random.randint(0, max_len - sig_len)\n",
    "            pad_end_len  =max_len - sig_len - pad_begin_len\n",
    "            pad_begin = torch.zeros((num_rows, pad_begin_len))\n",
    "            pad_end = torch.zeros((num_rows, pad_end_len))\n",
    "            sig = torch.cat((pad_begin, sig, pad_end), 1)\n",
    "        return sig, sr\n",
    "\n",
    "    @staticmethod\n",
    "    def time_shift(audio, shift_limit):\n",
    "        sig, sr = audio\n",
    "        _, sig_len = sig.shape\n",
    "        shift_amt = int(random.random() * shift_limit * sig_len)\n",
    "        return (sig.roll(shift_amt), sr)\n",
    "\n",
    "    @staticmethod\n",
    "    def spectrogram(audio, n_mels=64, n_fft=1024, hop_len=None):\n",
    "        sig, sr = audio\n",
    "        top_db = 80\n",
    "        spec = transforms.MelSpectrogram(sr, n_fft=n_fft, hop_length=hop_len, n_mels=n_mels)(sig)\n",
    "        spec = transforms.AmplitudeToDB(top_db=top_db)(spec)\n",
    "        return spec\n",
    "\n",
    "    def spectrogram_augment(spectrogram, max_mask_pct=0.1, n_freq_maks=1, n_time_masks=1):\n",
    "        _, n_mels, n_steps = spectrogram.shape\n",
    "        mask_value = spectrogram.mean()\n",
    "        aug_spec = spectrogram\n",
    "        freq_mask_param = max_mask_pct * n_mels\n",
    "        for _ in range(n_freq_maks):\n",
    "            aug_spec = transforms.FrequencyMasking(freq_mask_param)(aug_spec, mask_value)\n",
    "        time_mask_param = max_mask_pct * n_steps\n",
    "        for _ in range(n_time_masks):\n",
    "            aug_spec = transforms.TimeMasking(time_mask_param)(aug_spec, mask_value)\n",
    "        return aug_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoundDataset(Dataset):\n",
    "    def __init__(self, df, data_path):\n",
    "        self.df = df\n",
    "        self.data_path = str(data_path)\n",
    "        self.duration = 4000\n",
    "        self.sr = 44100\n",
    "        self.channel = 2\n",
    "        self.shift_pct = 0.4\n",
    "        self.ordinal_enc = OrdinalEncoder()\n",
    "        self.df[\"label\"] = self.ordinal_enc.fit_transform(df[[\"label\"]])\n",
    "        # self.y = torch.tensor(self.df[\"label\"])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        audio_file = self.data_path + self.df.loc[index, \"fname\"]\n",
    "        label = torch.tensor(self.df.loc[index, \"label\"], dtype=torch.long)\n",
    "\n",
    "        audio = AudioUtils.open(audio_file)\n",
    "        re_aud = AudioUtils.resample(audio, self.sr)\n",
    "        re_chan = AudioUtils.rechannel(re_aud, self.channel)\n",
    "        dur_aud = AudioUtils.pad_trunc(re_chan, self.duration)\n",
    "        shift_aud = AudioUtils.time_shift(dur_aud, self.shift_pct)\n",
    "        sgram = AudioUtils.spectrogram(shift_aud)\n",
    "        aug_sgram = AudioUtils.spectrogram_augment(sgram)\n",
    "        return aug_sgram, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16\n",
    "TRAIN_PATH = \"data/audio_train/\"\n",
    "TEST_PATH = \"data/audio_test/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = SoundDataset(data_train, TRAIN_PATH)\n",
    "test_ds = SoundDataset(data_test, TEST_PATH)\n",
    "\n",
    "train_dl = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_dl = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_10103/3897896954.py:18: DeprecationWarning: an integer is required (got type numpy.float64).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  label = torch.tensor(self.df.loc[index, \"label\"], dtype=torch.long)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[[-43.1036, -36.8363, -44.8570,  ..., -37.0609, -44.8570, -44.8570],\n",
       "          [-44.8570, -29.9207, -32.4493,  ..., -36.8845, -40.1252, -35.0314],\n",
       "          [-31.1404, -33.0701, -32.2485,  ..., -42.7650, -36.8694, -35.6922],\n",
       "          ...,\n",
       "          [-33.2584, -44.8570, -44.8570,  ..., -44.8570, -44.8570, -44.8165],\n",
       "          [-33.4945, -44.5604, -44.8570,  ..., -44.8570, -44.8570, -44.8570],\n",
       "          [-33.5546, -44.7317, -44.8570,  ..., -44.8570, -44.6854, -44.8570]],\n",
       " \n",
       "         [[-43.1036, -36.8363, -44.8570,  ..., -37.0609, -44.8570, -44.8570],\n",
       "          [-44.8570, -29.9207, -32.4493,  ..., -36.8845, -40.1252, -35.0314],\n",
       "          [-31.1404, -33.0701, -32.2485,  ..., -42.7650, -36.8694, -35.6922],\n",
       "          ...,\n",
       "          [-33.2584, -44.8570, -44.8570,  ..., -44.8570, -44.8570, -44.8165],\n",
       "          [-33.4945, -44.5604, -44.8570,  ..., -44.8570, -44.8570, -44.8570],\n",
       "          [-33.5546, -44.7317, -44.8570,  ..., -44.8570, -44.6854, -44.8570]]]),\n",
       " tensor(23))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioClassifier(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        conv_layers = []\n",
    "\n",
    "        # CONV BLOCK 1\n",
    "        self.conv_1 = torch.nn.Conv2d(2, 8, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n",
    "        self.relu_1 = torch.nn.ReLU()\n",
    "        self.batchnorm_1 = torch.nn.BatchNorm2d(8)\n",
    "        init.kaiming_normal_(self.conv_1.weight, a=0.1)\n",
    "        self.conv_1.bias.data.zero_()\n",
    "        conv_layers += [self.conv_1, self.relu_1, self.batchnorm_1]\n",
    "\n",
    "        # CONV BLOCK 2\n",
    "        self.conv_2 = torch.nn.Conv2d(8, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
    "        self.relu_2 = torch.nn.ReLU()\n",
    "        self.batchnorm_2 = torch.nn.BatchNorm2d(16)\n",
    "        init.kaiming_normal_(self.conv_2.weight, a=0.1)\n",
    "        self.conv_2.bias.data.zero_()\n",
    "        conv_layers += [self.conv_2, self.relu_2, self.batchnorm_2]\n",
    "\n",
    "        # CONV BLOCK 3\n",
    "        self.conv_3 = torch.nn.Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
    "        self.relu_3 = torch.nn.ReLU()\n",
    "        self.batchnorm_3 = torch.nn.BatchNorm2d(32)\n",
    "        init.kaiming_normal_(self.conv_3.weight, a=0.1)\n",
    "        self.conv_3.bias.data.zero_()\n",
    "        conv_layers += [self.conv_3, self.relu_3, self.batchnorm_3]\n",
    "\n",
    "        # CONV BLOCK 4\n",
    "        self.conv_4 = torch.nn.Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
    "        self.relu_4 = torch.nn.ReLU()\n",
    "        self.batchnorm_4 = torch.nn.BatchNorm2d(64)\n",
    "        init.kaiming_normal_(self.conv_4.weight, a=0.1)\n",
    "        self.conv_4.bias.data.zero_()\n",
    "        conv_layers += [self.conv_4, self.relu_4, self.batchnorm_4]\n",
    "\n",
    "        self.ap = torch.nn.AdaptiveAvgPool2d(output_size=1)\n",
    "        self.lin = torch.nn.Linear(in_features=64, out_features=LABELS_COUNT)\n",
    "\n",
    "        self.conv = torch.nn.Sequential(*conv_layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.ap(x)\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        x = self.lin(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AudioClassifier()\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "next(model.parameters()).device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(model, train_dl, num_epochs):\n",
    "  criterion = nn.CrossEntropyLoss()\n",
    "  optimizer = torch.optim.Adam(model.parameters(),lr=0.001)\n",
    "  scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=0.001,\n",
    "                                                steps_per_epoch=int(len(train_dl)),\n",
    "                                                epochs=num_epochs,\n",
    "                                                anneal_strategy='linear')\n",
    "\n",
    "  for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    correct_prediction = 0\n",
    "    total_prediction = 0\n",
    "\n",
    "    for i, data in enumerate(train_dl):\n",
    "        inputs, labels = data[0].to(device), data[1].to(device)\n",
    "\n",
    "        inputs_m, inputs_s = inputs.mean(), inputs.std()\n",
    "        inputs = (inputs - inputs_m) / inputs_s\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        _, prediction = torch.max(outputs,1)\n",
    "        correct_prediction += (prediction == labels).sum().item()\n",
    "        total_prediction += prediction.shape[0]\n",
    "\n",
    "    num_batches = len(train_dl)\n",
    "    avg_loss = running_loss / num_batches\n",
    "    acc = correct_prediction/total_prediction\n",
    "    print(f'Epoch: {epoch}, Loss: {avg_loss:.2f}, Accuracy: {acc:.2f}')\n",
    "\n",
    "  print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_10103/3897896954.py:18: DeprecationWarning: an integer is required (got type numpy.float64).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  label = torch.tensor(self.df.loc[index, \"label\"], dtype=torch.long)\n"
     ]
    }
   ],
   "source": [
    "training(model, train_dl, NUM_EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference (model, val_dl):\n",
    "  correct_prediction = 0\n",
    "  total_prediction = 0\n",
    "\n",
    "  with torch.no_grad():\n",
    "    for data in val_dl:\n",
    "      inputs, labels = data[0].to(device), data[1].to(device)\n",
    "\n",
    "      inputs_m, inputs_s = inputs.mean(), inputs.std()\n",
    "      inputs = (inputs - inputs_m) / inputs_s\n",
    "\n",
    "      outputs = model(inputs)\n",
    "\n",
    "      _, prediction = torch.max(outputs,1)\n",
    "      correct_prediction += (prediction == labels).sum().item()\n",
    "      total_prediction += prediction.shape[0]\n",
    "    \n",
    "  acc = correct_prediction/total_prediction\n",
    "  print(f'Accuracy: {acc:.2f}, Total items: {total_prediction}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference(model, test_dl)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d9a8acb4f733d3596df9f6fac9daff15e014d11794ebc65488d1c191c94698fd"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
